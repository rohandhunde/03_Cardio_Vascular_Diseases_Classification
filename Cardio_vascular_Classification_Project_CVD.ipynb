{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "MSa1f5Uengrz",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "bbFf2-_FphqN",
        "nqoHp30x9hH9",
        "bmKjuQ-FpsJ3",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohandhunde/Cardio_Vascular_Diseases_Classification/blob/main/Cardio_vascular_Classification_Project_CVD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Rohan Dhunde 1 **\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cardiovascular disease is a major public health concern worldwide, and it is the leading cause of death in many countries. This disease affects the heart and blood vessels and can lead to serious health consequences, including heart attacks, strokes, and heart failure. In order to prevent and treat cardiovascular disease, it is important to understand the risk factors associated with it.\n",
        "\n",
        "The Cardiovascular disease dataset is a comprehensive dataset that contains information about individuals with and without cardiovascular disease. This dataset includes various risk factors such as age, gender, blood pressure, cholesterol levels, and smoking status. The aim of the dataset is to identify patterns and risk factors that are associated with cardiovascular disease.\n",
        "\n",
        "One of the most important risk factors for cardiovascular disease is age. As people get older, their risk of developing cardiovascular disease increases. In addition to age, gender also plays a role in cardiovascular disease risk. Men are generally more likely to develop cardiovascular disease than women, although this difference decreases after menopause.\n",
        "\n",
        "Blood pressure is another important risk factor for cardiovascular disease. High blood pressure, also known as hypertension, puts extra strain on the heart and blood vessels and can lead to damage over time. Cholesterol levels are also important risk factors for cardiovascular disease. High levels of LDL cholesterol, often referred to as \"bad\" cholesterol, can lead to a buildup of plaque in the arteries, which can increase the risk of heart attacks and strokes.\n",
        "\n",
        "Smoking is another major risk factor for cardiovascular disease. Smoking damages the blood vessels and can lead to the buildup of plaque, which increases the risk of heart attacks and strokes. Additionally, smoking can cause inflammation, which can contribute to the development of cardiovascular disease.\n",
        "\n",
        "By analyzing the Cardiovascular disease dataset, researchers can gain insights into how these risk factors interact and potentially develop new methods for preventing or treating cardiovascular disease. For example, they may identify certain combinations of risk factors that are particularly dangerous and develop targeted interventions to reduce the risk of cardiovascular disease in these individuals.\n",
        "\n",
        "In addition to identifying risk factors, the Cardiovascular disease dataset can also be used to explore the effectiveness of various treatments for cardiovascular disease. For example, researchers may analyze the data to determine which medications or lifestyle interventions are most effective at reducing the risk of cardiovascular disease or improving outcomes for those who have already developed the disease.\n",
        "\n",
        "Overall, the Cardiovascular disease dataset is a valuable resource for researchers and healthcare professionals who are working to prevent and treat cardiovascular disease. By understanding the risk factors associated with this disease and identifying effective interventions, we can help reduce the burden of cardiovascular disease and improve health outcomes for individuals around the world."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/rohandhunde/ML_Cardio_Vascular_Diseases_Classification/blob/main/ML_Classification_Project_CVD.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cardiovascular diseases are a leading cause of mortality and morbidity worldwide. The problem statement for cardiovascular diseases involves understanding the risk factors, causes, and consequences of these diseases, as well as developing effective prevention, diagnosis, and treatment strategies. The goal is to reduce the incidence and impact of cardiovascular diseases on individuals, communities, and healthcare systems, and to improve overall cardiovascular health and well-being..**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.stats import uniform\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn import svm\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
        "from sklearn.metrics import make_scorer, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBRFClassifier\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Replace the file link with the link to your own file\n",
        "url = 'https://drive.google.com/file/d/1nbysKW2BDaBdOZEYJbYMrrdRs-aL3P2q/view?usp=sharing'\n",
        "\n",
        "# Extract the file ID from the link\n",
        "file_id = url.split('/')[-2]\n",
        "\n",
        "# Generate a download link for the file\n",
        "download_link = 'https://drive.google.com/uc?id=' + file_id\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(download_link)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first look of the dataset\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "\n",
        "# Visualize missing values in the dataframe\n",
        "msno.matrix(df)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset under consideration has a shape of (3390, 17) and is missing values in several columns, namely education, clgsperday, BpMeds, totChol, BMI, glucose. The number of missing values in each column varies, with education having 87 missing values, clgsperday having 22 missing values, BpMeds having 44 missing values, totChol having 38 missing values, BMI having 14 missing values, heartRate having 1 missing value, and glucose having 304 missing values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# column of the dataset\n",
        "df.columns"
      ],
      "metadata": {
        "id": "AxWfouEQFfh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description\n",
        "\n",
        "\n",
        "\n",
        "*  id: Unique identifier for each observation.\n",
        "* age: Age of the participant in years.\n",
        "* education: Level of education completed by the participant, with values ranging from 1 (less than high school) to 4 (college graduate).\n",
        "* sex: Gender of the participant, with values of \"F\" (female) or \"M\" (male).\n",
        "* is_smoking: Smoking status of the participant, with values of \"NO\" (not currently smoking) or \"YES\" (currently smoking).\n",
        "* cigsPerDay: Number of cigarettes smoked per day by the participant.\n",
        "* BPMeds: Whether the participant is taking blood pressure medication, with values of 0 (not taking medication) or 1 (taking medication).\n",
        "* prevalentStroke: Whether the participant has a history of stroke, with values of 0 (no history) or 1 (history of stroke).\n",
        "* prevalentHyp: Whether the participant has a history of hypertension, with values of 0 (no history) or 1 (history of hypertension).\n",
        "* diabetes: Whether the participant has diabetes, with values of 0 (no diabetes) or 1 (diabetes).\n",
        "* totChol: Total cholesterol level in mg/dL.\n",
        "* sysBP: Systolic blood pressure in mmHg.\n",
        "* diaBP: Diastolic blood pressure in mmHg.\n",
        "* BMI: Body mass index, calculated as weight (kg) / height (m)^2.\n",
        "* heartRate: Resting heart rate in beats per minute.\n",
        "* glucose: Fasting blood glucose level in mg/dL.\n",
        "* TenYearCHD: Ten-year risk of coronary heart disease, with values of 0 (low risk) or 1 (high risk)."
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the unique values\n",
        "print(df.nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "sk6__VdcH3ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "n4ruZn6nLwqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "w7CcG6ayo3Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Categorical columns to convert to numeric\n",
        "cat_cols = ['education', 'sex', 'is_smoking', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'TenYearCHD']\n",
        "\n",
        "# Create LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit and transform the categorical columns into numeric\n",
        "for col in cat_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting first look after applying label encoder\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h7NsgMR_q6dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In summary, converting categorical columns to numeric using one-hot encoding allows us to include these columns in machine learning models. However, we need to make sure to drop any extra columns that are not necessary for our analysis to avoid adding unnecessary complexity to our models."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Countplot"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# creating the barplot\n",
        "sns.set(style=\"darkgrid\")\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "sns.countplot(x=\"TenYearCHD\", hue=\"education\", data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the countplot is a good choice for showing the count or frequency of observations in different categories. In this case, we are interested in showing the count of individuals with and without heart disease (TenYearCHD equal to 1 or 0) based on their education level (education)"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the countplot of heart disease incidence (TenYearCHD equal to 1 or 0) across education levels (education), we can observe that the number of individuals with heart disease decreases as the level of education increases. Therefore, there appears to be a negative correlation between education level and heart disease incidence. However, further analysis and evidence would be needed to draw any definitive conclusions about this relationship."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is difficult to determine whether the insights gained from a single visualization, such as the countplot of heart disease incidence across education levels, will have a positive business impact without further context and analysis.\n",
        "\n",
        "However, if we assume that there is a negative correlation between education level and heart disease incidence, then it may be possible to use this insight to create positive business impact by targeting educational interventions or health promotion campaigns towards individuals with lower education levels.\n",
        "\n",
        "On the other hand, if there were insights that lead to negative growth, they would likely involve identifying patterns or relationships that suggest a decline or loss in business opportunities or performance. For example, if the countplot revealed a negative correlation between customer satisfaction ratings and product features, this could lead to negative growth if the company does not take steps to address the underlying issues and improve customer satisfaction.\n",
        "\n",
        "Therefore, the impact of insights on business growth will depend on the specific context, the quality and relevance of the insights, and the actions taken based on those insights."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - **BoxPlot**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Crete the barplot\n",
        "sns.set(style=\"darkgrid\")\n",
        "tips = sns.load_dataset(\"tips\")\n",
        "sns.barplot(x=\"TenYearCHD\",y=\"cigsPerDay\", data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##### 1 . Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I want to compare the values of a categorical variable by showing their frequencies or counts. Barplots are particularly useful for visualizing the distribution of categorical data and comparing the values of different categories."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the plot, it appears that there is a positive association between the number of cigarettes smoked per day and the risk of heart disease. As the number of cigarettes smoked per day increases, the proportion of individuals with heart disease also increases. This suggests that smoking is a significant risk factor for heart disease, and individuals who smoke should consider quitting or reducing their cigarette intake to decrease their risk."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights regarding the positive association between cigarette consumption and the risk of heart disease may have a positive business impact for companies that promote smoking cessation or provide products and services to support quitting smoking, such as nicotine replacement therapies, counseling, or support groups. These insights may also be useful for health insurance companies or employers who offer wellness programs to encourage smoking cessation and reduce the risk of heart disease among their members or employees.\n",
        "\n",
        "However, if a company produces or sells cigarettes, these insights may lead to negative growth as more individuals become aware of the health risks associated with smoking and decide to quit or reduce their cigarette intake. This may result in a decline in demand for cigarettes and a decrease in sales revenue. Additionally, if the negative impact of smoking on health and the associated healthcare costs become more widely recognized, this may lead to increased regulations or taxes on tobacco products, further reducing demand and profitability for companies in the tobacco industry."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Barplot"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# bar Plot\n",
        "x = df[\"is_smoking\"]\n",
        "y = df[\"TenYearCHD\"]\n",
        "\n",
        "sns.barplot(x=x, y=y)"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A barplot is a good choice for this scenario because it allows us to compare the average value of TenYearCHD for different categories of is_smoking. In a barplot, each category of is_smoking is represented by a bar, and the height of the bar corresponds to the average value of TenYearCHD for that category."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the bar plot created for the relationship between is_smoking and TenYearCHD, it appears that individuals who smoke are at a higher risk of developing coronary heart disease (CHD) compared to those who do not smoke. This is indicated by the significantly higher average value of TenYearCHD for smokers compared to non-smokers. Therefore, smoking could be considered a risk factor for CHD in the population being studied. However, additional analyses and considerations of other factors are necessary to establish a more comprehensive understanding of the relationship between smoking and CHD."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the analysis of the relationship between is_smoking and TenYearCHD may help in creating a positive business impact in several ways. For example, if the business is a health insurance company, the insights could inform the development of insurance policies that incentivize smoking cessation or provide higher coverage for non-smokers. Similarly, if the business is a pharmaceutical company, the insights could inform the development of drugs that target the specific risk factors for CHD in smokers.\n",
        "\n",
        "However, it is also possible that the insights could lead to negative growth in some cases. For example, if the business is a tobacco company, the insights could indicate that their products are associated with a higher risk of CHD, which could lead to decreased demand for their products and negative growth. Similarly, if the business is a healthcare provider that specializes in treating CHD, the insights could indicate that the number of patients with CHD is likely to increase due to smoking, which could lead to increased demand for their services but also potentially negative growth in terms of the overall health of the population.\n",
        "\n",
        "Therefore, it is important to carefully consider the context and implications of the insights gained from the analysis before determining their potential impact on the business."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Barplot"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Create the barplot\n",
        "\n",
        "x = df[\"totChol\"]\n",
        "y = df[\"TenYearCHD\"]\n",
        "\n",
        "sns.barplot(x=x, y=y)"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to compare the values of a categorical variable by showing their frequencies or counts. Barplots are particularly useful for visualizing the distribution of categorical data and comparing the values of different categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When cholesterol levels in the blood increase, the risk of developing heart disease also increases. High levels of cholesterol can lead to the buildup of fatty deposits in the arteries, which can narrow and eventually block blood flow to the heart, increasing the risk of heart attacks and other cardiovascular problems.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the insights gained from the information about cholesterol and heart disease may have a negative impact on certain businesses, particularly those in the food industry that produce and sell products high in cholesterol, such as fatty meats and dairy products. Consumers who are aware of the health risks associated with high cholesterol may be less likely to purchase these products, which could lead to a decline in sales and negative growth for these businesses. However, it is important to note that there are many other factors that can affect business growth, and the impact of cholesterol on any specific business may vary depending on a range of factors."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Lineplot"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# create a line plot\n",
        "sns.lineplot(x=df[\"glucose\"],y=df[\"TenYearCHD\"])"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I may have chosen a line plot because it is particularly useful for visualizing trends and changes over time, which can be helpful in identifying patterns or relationships in the data. Line plots can also be useful for comparing multiple groups or variables, as they can be plotted on the same graph with different colors or line styles."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elevated levels of glucose in the blood can increase the risk of developing cardiovascular diseases, including heart disease. As glucose levels increase, the risk of cardiovascular problems also tends to increase, highlighting the importance of maintaining healthy glucose levels to reduce the risk of heart disease."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the information about glucose and cardiovascular disease may have both positive and negative impacts on businesses. On one hand, businesses that produce and sell products that promote healthy glucose levels, such as low-sugar or sugar-free foods, may benefit from increased consumer demand. Additionally, companies that specialize in glucose monitoring devices or diabetes management products may see increased sales.\n",
        "\n",
        "On the other hand, businesses that produce and sell products high in sugar, such as sweetened beverages or high-sugar snacks, may see decreased sales as consumers become more aware of the health risks associated with elevated glucose levels. Additionally, healthcare providers and insurers may face increased costs as a result of treating individuals with cardiovascular disease and related complications, which could have a negative impact on their bottom line.\n",
        "\n",
        "Overall, the impact of insights gained from the relationship between glucose and cardiovascular disease on specific businesses may vary depending on their industry, products, and target market."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Lineplot"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "sns.barplot(x=df[\"BMI\"], y=df[\"age\"])\n",
        "plt.title(\"BMI vs. Age\")\n",
        "plt.xlabel(\"BMI\")\n",
        "plt.ylabel(\"Age\")\n",
        "plt.show()\n",
        "\n",
        "# plot 2: lineplot of age vs. 10-Year CHD\n",
        "sns.lineplot(x=df[\"age\"], y=df[\"TenYearCHD\"])\n",
        "plt.title(\"Age vs. 10-Year CHD\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"10-Year CHD\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I may have chosen a line plot because it is particularly useful for visualizing trends and changes over time, which can be helpful in identifying patterns or relationships in the data. Line plots can also be useful for comparing multiple groups or variables, as they can be plotted on the same graph with different colors or line styles."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first plot shows a positive correlation between BMI and age, where BMI tends to increase with age, particularly between the ages of 30 and 70. The second plot shows that there is an increasing trend in the risk of heart disease with increasing age. Therefore, age appears to be a factor that influences both BMI and the risk of heart disease."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the analysis of the cardiovascular disease dataset may be useful in the healthcare industry to improve patient outcomes and reduce the incidence of heart disease. For example, the identification of risk factors such as age can help healthcare providers to develop more effective prevention and treatment strategies.\n",
        "\n",
        "As for negative growth, there may be potential negative impacts on businesses in the healthcare industry that rely on the treatment of heart disease, as the identification and prevention of risk factors may reduce the number of patients requiring treatment. However, the benefits of reducing the incidence of heart disease, improving patient outcomes, and reducing healthcare costs are likely to outweigh any negative impacts on businesses."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Chart - **Boxplot**"
      ],
      "metadata": {
        "id": "x5neZy0IAmw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another figure\n",
        "# Scatter with positive examples\n",
        "x = df[\"sex\"]\n",
        "y = df[\"TenYearCHD\"]\n",
        "\n",
        "sns.barplot(x=x, y=y)"
      ],
      "metadata": {
        "id": "gplN056DAvJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "NDNt-nEmAvfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I want to compare the values of a categorical variable by showing their frequencies or counts. Barplots are particularly useful for visualizing the distribution of categorical data and comparing the values of different categories."
      ],
      "metadata": {
        "id": "gkLBa1BLA4Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_c2i8LiFA7Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "according to above plot we can see the male counts are greatter than the femaleBased on the bar plot, which shows the relationship between TenYearCHD and sex, it appears that there are more males than females who have heart disease. The data indicates that the count of females with heart disease is lower than the count of males with heart disease."
      ],
      "metadata": {
        "id": "fZ2-jZGoBAyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M-dnRge1BJhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " this above  information could help businesses to design more effective prevention strategies and interventions that are tailored to specific gender-based risk factors. For example, they could design targeted marketing campaigns to raise awareness of the risk factors and preventative measures that are specific to men and women.\n",
        "\n",
        "On the negative side, if businesses rely solely on the gender-based risk factor to design interventions, they may overlook other important factors that contribute to the risk of heart disease. Therefore, it is important to conduct further analysis and research to identify other potential risk factors and develop more comprehensive strategies to prevent heart disease."
      ],
      "metadata": {
        "id": "3n6CQfNSA2Y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the correlation heatmap is a useful tool to visualize the correlation between different variables in a dataset. It provides a color-coded representation of the correlation coefficients between pairs of variables, making it easy to identify patterns and relationships. In addition, it can help in identifying potential multicollinearity issues and selecting features for machine learning models."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statement suggests that when comparing a column with itself, there will always be a perfect match and a correlation coefficient of 1 or 100%. This is because each value in the column is perfectly correlated with itself. This information is useful to keep in mind when analyzing the correlation between variables, as it highlights the importance of looking for correlations between different variables rather than simply comparing the same variable to itself."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df, hue='TenYearCHD')"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would explain that a Pair Plot is a powerful visualization tool that allows us to plot pairwise relationships between multiple variables in a single plot. It helps us to quickly identify the relationships and patterns between different variables in a dataset, making it an ideal choice for exploratory data analysis. The plot shows scatter plots for all possible pairs of variables along with histograms for each variable along the diagonal, making it easier to spot any correlations, trends or anomalies.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statement suggests that there are strong correlations between the different columns in the dataset, and that these relationships can be further explored through visualizations. It highlights the importance of using data visualization techniques to gain insights and identify patterns and relationships in the data. By visualizing the data, we can better understand the complex interplay between the different variables and use this information to inform further analysis and decision-making.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# checking the missing values\n",
        "df.isna().sum()\n",
        "# create a SimpleImputer object\n",
        "imputer = SimpleImputer(strategy='mean') # you can also use 'median' or 'most_frequent' as strategy\n",
        "\n",
        "# select columns with missing values\n",
        "cols_with_missing = ['cigsPerDay', 'totChol', 'BMI', 'heartRate', 'glucose']\n",
        "\n",
        "# fill missing values in the selected columns\n",
        "df[cols_with_missing] = imputer.fit_transform(df[cols_with_missing])\n",
        "\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first look after handling the missing values\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HroPMWqFFdHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You used SimpleImputer to fill all the missing values in the dataset using the 'mean' strategy. The mean strategy is used when the missing values are numeric and the assumption is that the missing values are most likely similar to the mean of the available values. SimpleImputer replaces the missing values with the mean value of the corresponding column. This imputation technique is commonly used because it is simple to implement and is less likely to cause overfitting compared to other techniques."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check the discrete and continuous features\n",
        "categorical_features = [i for i in df.columns if df[i].nunique()<=4]\n",
        "numeric_features = [i for i in df.columns if i not in categorical_features]\n",
        "\n",
        "print(categorical_features)\n",
        "print(numeric_features)"
      ],
      "metadata": {
        "id": "G4vgLEMbMWDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Visualizing the distributions.\n",
        "plt.figure(figsize=(18,12))\n",
        "for n,column in enumerate(numeric_features):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.distplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "xBezc7ZqMQ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization is the easiest way to have an inference about the overall data and the outliers.\n",
        "plt.figure(figsize=(18,12))\n",
        "for n,column in enumerate(numeric_features):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.boxplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "rtLdPz2oM4Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "techniques used in your analysis of the cardiovascular diseases dataset. However, in general, some common outlier treatment techniques include removing the outliers, winsorizing (replacing the outliers with the nearest extreme value), or transforming the data to reduce the impact of outliers. The choice of technique would depend on the specific dataset, the nature of the outliers, and the goals of the analysis. It is important to carefully consider the impact of outliers on the analysis and choose an appropriate treatment technique to minimize their influence."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q860bReeGqTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# initialize LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# encode education column\n",
        "df['education'] = le.fit_transform(df['education'])\n",
        "\n",
        "# encode sex column\n",
        "df['sex'] = le.fit_transform(df['sex'])\n",
        "\n",
        "# encode is_smoking column\n",
        "df['is_smoking'] = le.fit_transform(df['is_smoking'])\n",
        "\n",
        "# encode BPMeds column\n",
        "df['BPMeds'] = le.fit_transform(df['BPMeds'].astype(str))\n",
        "\n",
        "# encode prevalentStroke column\n",
        "df['prevalentStroke'] = le.fit_transform(df['prevalentStroke'].astype(str))\n",
        "\n",
        "# encode prevalentHyp column\n",
        "df['prevalentHyp'] = le.fit_transform(df['prevalentHyp'].astype(str))\n",
        "\n",
        "# encode diabetes column\n",
        "df['diabetes'] = le.fit_transform(df['diabetes'].astype(str))\n",
        "\n",
        "# encode TenYearCHD column\n",
        "df['TenYearCHD'] = le.fit_transform(df['TenYearCHD'])"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The categorical encoding techniques used in the above code are Label Encoding. Label Encoding is used to convert categorical features into numerical values so that they can be used as input for machine learning algorithms. The LabelEncoder class from the scikit-learn library is used to perform label encoding on the specified columns."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# create new feature 'bmi_range'\n",
        "df['bmi_range'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 35, 40, np.inf], labels=['underweight', 'normal', 'overweight', 'obese class I', 'obese class II', 'obese class III'])\n",
        "\n",
        "# create new feature 'age_range'\n",
        "df['age_range'] = pd.cut(df['age'], bins=[0, 30, 40, 50, 60, 70, 80], labels=['20s', '30s', '40s', '50s', '60s', '70s+'])\n",
        "\n",
        "# drop highly correlated features\n",
        "df = df.drop(['sysBP', 'diaBP', 'cigsPerDay'], axis=1)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the age_range column\n",
        "df = df.drop('age_range', axis=1)"
      ],
      "metadata": {
        "id": "G6z_SHV7O2V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qL-2y4D0JqeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the age_range column\n",
        "df = df.drop('bmi_range', axis=1)\n"
      ],
      "metadata": {
        "id": "QUlRweqLTLL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "r6pyD2x5TMsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# define your target variable\n",
        "target = 'TenYearCHD'\n",
        "\n",
        "# define your features\n",
        "features = ['age', 'education', 'sex', 'is_smoking' , 'BPMeds',\n",
        "       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol',  'BMI', 'heartRate',\n",
        "       'bmi_range','age_range']\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i inetealize the targer column and features"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "because the one is the dependend variable and one is the independent variable and its is the important to each other to get good predictions"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# initialize LabelEncoder\n",
        "le = LabelEncoder()\n",
        "# encode education column\n",
        "df['education'] = le.fit_transform(df['education'])\n",
        "# encode sex column\n",
        "df['sex'] = le.fit_transform(df['sex'])\n",
        "# encode is_smoking column\n",
        "df['is_smoking'] = le.fit_transform(df['is_smoking'])\n",
        "# encode BPMeds column\n",
        "df['BPMeds'] = le.fit_transform(df['BPMeds'].astype(str))\n",
        "# encode prevalentStroke column\n",
        "df['prevalentStroke'] = le.fit_transform(df['prevalentStroke'].astype(str))\n",
        "# encode prevalentHyp column\n",
        "df['prevalentHyp'] = le.fit_transform(df['prevalentHyp'].astype(str))\n",
        "# encode diabetes column\n",
        "df['diabetes'] = le.fit_transform(df['diabetes'].astype(str))\n",
        "# encode TenYearCHD column\n",
        "df['TenYearCHD'] = le.fit_transform(df['TenYearCHD'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Define the target variable and features\n",
        "target = 'TenYearCHD'\n",
        "features = ['age', 'education', 'sex', 'is_smoking' , 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol',  'BMI', 'heartRate']\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the feature data\n",
        "scaled_data = scaler.fit_transform(df[features])\n",
        "\n",
        "# Convert the scaled data to a pandas DataFrame\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=features)\n",
        "\n",
        "# Concatenate the scaled features with the target variable\n",
        "scaled_df[target] = df[target]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i uses the standerd scaler to scale the dataset because it is the best way to scale the data"
      ],
      "metadata": {
        "id": "IwgNgUd6aVas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in general, dimensionality reduction is often useful in machine learning when dealing with high-dimensional datasets because it can help to reduce the computational complexity of the model and remove redundant or irrelevant features that may negatively affect the model's performance."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "# Define your target variable\n",
        "target = 'TenYearCHD'\n",
        "\n",
        "# Define your features\n",
        "features = ['age', 'education', 'sex', 'is_smoking', 'BPMeds',\n",
        "            'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'BMI',\n",
        "            'heartRate', 'glucose']\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_norm = scaler.fit_transform(X)\n",
        "\n",
        "# Instantiate PCA with the number of components desired\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit and transform the data\n",
        "X_pca = pca.fit_transform(X_norm)\n",
        "\n",
        "# Concatenate the transformed data and the target variable into a DataFrame\n",
        "df_pca = pd.concat([pd.DataFrame(X_pca, columns=['PCA1', 'PCA2']), y], axis=1)\n",
        "\n",
        "# Visualize the results\n",
        "sns.scatterplot(x='PCA1', y='PCA2', hue=target, data=df_pca)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dimensionality reduction technique used in the code is Principal Component Analysis (PCA). PCA is a commonly used technique for reducing the dimensionality of high-dimensional datasets by projecting the data onto a lower-dimensional space while preserving as much of the original variance as possible.\n",
        "\n",
        "In above  code, PCA is used to reduce the dimensionality of the feature set to two principal components, which can then be visualized in a scatter plot. This helps to identify any patterns or relationships between the features and the target variable. PCA is a good choice for this dataset because it can handle high-dimensional datasets and identify the most important features for predicting the target variable."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# instanciate the variable X and y\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# set random randomness\n",
        "np.random.seed(42)\n",
        "\n",
        "# spliting data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets plot the shape of the splitted data\n",
        "X_train.shape, X_test.shape, y_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "GGoQkRj4hdVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i use the ratio of the spliting that is 80:20 where 80% are the training data and 20 is the testing data"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the data imbalence\n",
        "value_counts = df['TenYearCHD'].value_counts()\n",
        "print(value_counts)"
      ],
      "metadata": {
        "id": "a5680okmi90n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Handling Imbalanced Dataset (If needed)\n",
        "# # Use SMOTE to oversample the minority class\n",
        "# smote = SMOTE()\n",
        "# X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "# X_train.shape,y_train.shape\n",
        "\n",
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Use SMOTE to oversample the minority class\n",
        "smote = SMOTE()\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the SMOTE technique to handle the imbalance dataset. SMOTE stands for Synthetic Minority Over-sampling Technique, and it is a widely used technique for oversampling the minority class in imbalanced datasets. The reason for using SMOTE is to address the class imbalance problem, where the number of instances in one class is significantly lower than the other class. SMOTE generates synthetic samples by creating new instances of the minority class that are similar to existing instances. This technique helps to improve the performance of the classification model by balancing the dataset and ensuring that the model is not biased towards the majority class."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Initialize Random Forest classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Fit Random Forest classifier to training data\n",
        "lr.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "lr.score(X_test, y_pred)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy , precision, recall, f1"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy score is 0.6165, indicating that 61.65% of the predictions made by the model are correct.\n",
        "\n",
        "The precision score is 0.1753, which means that only 17.53% of the positive predictions made by the model are correct.\n",
        "\n",
        "The recall score is 0.4536, indicating that only 45.36% of the actual positive values were correctly identified by the model.\n",
        "\n",
        "The F1 score is 0.2529, which is a harmonic mean of precision and recall, and combines both scores. It shows that the overall performance of the model is low."
      ],
      "metadata": {
        "id": "mvYVip6F8O3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metric scores\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Create a bar chart\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel(\"Evaluation Metric\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metric Scores\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(\"Precision: {:.4f}\".format(precision))\n",
        "print(\"Recall: {:.4f}\".format(recall))\n",
        "print(\"F1 Score: {:.4f}\".format(f1))"
      ],
      "metadata": {
        "id": "gokMhErBOWTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter"
      ],
      "metadata": {
        "id": "D2MgkpF0LGju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create logistic regression object\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "params = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(lr, params, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on test set with best hyperparameters and evaluate model performance\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "_MlQ3OECLJa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y_test,y_pred))\n",
        "print(precision_score(y_test,y_pred))\n",
        "print(recall_score(y_test,y_pred))\n",
        "print(f1_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "wl7UNMMdLPHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Create a bar plot\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "values = [acc, prec, rec, f1]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(metrics, values)\n",
        "ax.set_ylim([0, 1])\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Metrics')\n",
        "plt.show()\n",
        "\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "print(precision_score(y_test,y_pred))\n",
        "print(recall_score(y_test,y_pred))\n",
        "print(f1_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "I1O4vnvFLRxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap of the confusion matrix\n",
        "# Assuming y_test and y_pred are defined\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oFa5ncuVLWVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  y_test and y_pred are your true labels and predicted labels respectively\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr, tpr, lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XCjTHCME0IS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i have used Grid Search CV for hyperparameter tuning. Grid Search CV is a widely used hyperparameter optimization technique in machine learning. It performs an exhaustive search over a specified hyperparameter grid to find the optimal set of hyperparameters that gives the best performance for the given evaluation metric."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is an improvement in the model's performance after hyperparameter tuning. Here are the updated evaluation metric scores:\n",
        "\n",
        "Before hyperparameter tuning:\n",
        "\n",
        "Accuracy: 0.6165\n",
        "Precision: 0.1753\n",
        "Recall: 0.4536\n",
        "F1 Score: 0.2529\n",
        "\n",
        "After hyperparameter tuning:\n",
        "\n",
        "Accuracy: 0.6430\n",
        "Precision:0.2040\n",
        "Recall:0.5154\n",
        "F1 Score:0.2923\n",
        "As we can see, all evaluation metrics have improved after hyperparameter tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "# Initialize support vector classifier\n",
        "svc = SVC(random_state= 0,probability=True)\n",
        "# Fit the Algorithm\n",
        "# Fit Random Forest classifier to training data\n",
        "svc.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = svc.predict(X_test)\n",
        "svc.score(X_test,y_pred)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metric scores\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Create a bar chart\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel(\"Evaluation Metric\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metric Scores\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(\"Precision: {:.4f}\".format(precision))\n",
        "print(\"Recall: {:.4f}\".format(recall))\n",
        "print(\"F1 Score: {:.4f}\".format(f1))"
      ],
      "metadata": {
        "id": "6sj4KBJJCXmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc = SVC(random_state=0, probability=True)\n",
        "\n",
        "# Set up the parameter grid to search over\n",
        "param_grid = {\n",
        "    'C': [0.1, 1.0],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'degree': [2, 3],\n",
        "    'gamma': ['scale']\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "\n",
        "# Print the best score\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "qHdPt1X-MOHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy Score for SVM: \", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision Score for SVM: \", precision_score(y_test, y_pred))\n",
        "print(\"Recall Score for SVM: \", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score for SVM: \", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "GmulnDmfo_11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metric scores\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Create a bar chart\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel(\"Evaluation Metric\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metric Scores\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Accuracy Score for SVM: \", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision Score for SVM: \", precision_score(y_test, y_pred))\n",
        "print(\"Recall Score for SVM: \", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score for SVM: \", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Mw6NcY3Dzijq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap of the confusion matrix\n",
        "# Assuming y_test and y_pred are defined\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_CW39ZPGzlEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr, tpr, lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8Zq-KjjhKLLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i have used GridSearchCV for hyperparameter tuning. Grid Search CV is a widely used hyperparameter optimization technique in machine learning. It performs an exhaustive search over a specified hyperparameter grid to find the optimal set of hyperparameters that gives the best performance for the given evaluation metric."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " no improvement in the model's performance after hyperparameter tuning. The scores for Accuracy, Precision, Recall, and F1 Score remained the same before and after tuning. Therefore, there is no improvement to note down in the Evaluation metric score chart."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model's accuracy before hyperparameter tuning was 0.6224, and after tuning, it increased to 0.8333. The precision decreased from 0.2151 to 0.1364, indicating that the model now correctly identifies only 13.64% of the positive samples. The recall also decreased from 0.6186 to 0.0309, meaning that the model now correctly identifies only 3.09% of all actual positive samples. The business impact of the model depends on the specific application, but high accuracy, precision, and recall are generally desirable for medical diagnosis to avoid false diagnoses and incorrect treatment plans."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Initialize Random Forest classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Fit Random Forest classifier to training data\n",
        "rfc.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rfc.predict(X_test)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the score of the test set\n",
        "rfc.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "BQmQ4LhQrUKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy , precision, recall, f1"
      ],
      "metadata": {
        "id": "3qFKz4tfvxpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metric scores\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Create a bar chart\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel(\"Evaluation Metric\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metric Scores\")\n",
        "plt.show()\n",
        "\n",
        "# Print evaluation metrics scores\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(rfc, param_grid, cv=5)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print('Best hyperparameters:', best_params)\n",
        "\n",
        "# Create a new Random Forest classifier with the best hyperparameters\n",
        "rfc_best = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
        "                                   max_depth=best_params['max_depth'],\n",
        "                                   min_samples_split=best_params['min_samples_split'],\n",
        "                                   min_samples_leaf=best_params['min_samples_leaf'])\n",
        "\n",
        "# Fit the best Random Forest classifier to the training data\n",
        "rfc_best.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rfc_best.predict(X_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print evaluation metrics scores\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "Y7EaABkAHxiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [\"accuracy\", \"precision\", \"recall\", \"F1 Score\"]\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel(\"Evaluation Metric\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metric Scores\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Print evaluation metrics scores\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "SUhs9MFJP9YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap of the confusion matrix\n",
        "# Assuming y_test and y_pred are defined\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tfu0wVlS5qs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming y_test and y_pred are your true labels and predicted labels respectively\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr, tpr, lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MW6M-Ebr5tKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i have used GridSearchCV for hyperparameter tuning. Grid Search CV is a widely used hyperparameter optimization technique in machine learning. It performs an exhaustive search over a specified hyperparameter grid to find the optimal set of hyperparameters that gives the best performance for the given evaluation metric."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No improvement in the model's performance after hyperparameter tuning. The scores for Accuracy, Precision, Recall, and F1 Score remained the same before and after tuning. Therefore, there is no improvement to note down in the Evaluation metric score chart.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To determine which evaluation metrics would have a positive business impact, it is important to consider the specific goals and objectives of the business. Generally, accuracy, precision, recall, and F1-score are commonly used evaluation metrics for classification tasks.\n",
        "\n",
        "In this case, Model 1 has the highest accuracy of 0.643, but its precision and recall values are relatively low. Model 2 has a slightly lower accuracy of 0.622 but has higher recall and F1-score values, indicating that it correctly identifies more positive instances. On the other hand, Model 3 has a higher accuracy of 0.748 but very low precision and recall values, which suggests that it misses a significant number of actual positive instances and incorrectly classifies negative instances as positive.\n",
        "\n",
        "Considering these evaluation metrics and confusion matrices, it seems that Model 2 would have the most positive business impact in this case as it has higher recall and F1-score values, which means it correctly identifies more positive instances while minimizing the number of false negatives."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the all models side by side after hyperparameter tunning\n",
        "# Define the scores for three different models\n",
        "model1_scores = [0.64, 0.20, 0.51, 0.29]\n",
        "model2_scores = [0.62, 0.21, 0.61, 0.31]\n",
        "model3_scores = [0.74, 0.18, 0.22, 0.20]\n",
        "\n",
        "\n",
        "# Define the labels for each score\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "\n",
        "# Define the positions of the bars on the x-axis\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "# Define the width of each bar\n",
        "width = 0.25\n",
        "\n",
        "# Create a subplot with a single figure\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot the bars for each model and each score\n",
        "rects1 = ax.bar(x - width, model1_scores, width, label='Logistics Regression')\n",
        "rects2 = ax.bar(x, model2_scores, width, label='SVC')\n",
        "rects3 = ax.bar(x + width, model3_scores, width, label='RandomForestClassifier')\n",
        "\n",
        "# Add labels, title, and legend to the plot\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Comparison of Model Performance')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "# Add the values of each bar above the bars\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.2f}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z5j8azsIibyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can conclude the following:\n",
        "\n",
        "\n",
        "*    LinearRegression has an accuracy of 0.643 and relatively low precision and recall values, indicating that it has a high number of false negatives and misses actual positive instances.\n",
        "\n",
        "*   Support Vector  Classifier (SVC) it  has an accuracy of 0.622 but higher recall and F1-score values compared to Model 1, indicating that it correctly identifies more positive instances while minimizing the number of false negatives.\n",
        "\n",
        "*   RandomForestClassifier has the highest accuracy of 0.748, but very low precision and recall values, indicating that it misses a significant number of actual positive instances and incorrectly classifies negative instances as positive.\n",
        "\n",
        "*   The ROC curve areas for all three models are relatively low, with Support Vector  Classifier (SVC) having the highest value of 0.62.\n",
        "\n",
        "*   Based on the evaluation metrics and confusion matrices, Support Vector  Classifier (SVC) model appears to have the most positive business impact in this case as it correctly identifies more positive instances while minimizing the number of false negatives.\n",
        "\n",
        "\n",
        "\n",
        "In summary, the given data suggests that Support Vector  Classifier (SVC) has the most favorable performance among the three models, and it would be the preferred choice for this classification task. However, the specific choice of the model would also depend on the business context and the specific goals and requirements of the classification task."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! I have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}